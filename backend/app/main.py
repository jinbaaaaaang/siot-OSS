# -*- coding: utf-8 -*-
from typing import List, Optional, Dict
import time
import asyncio
import concurrent.futures
from pathlib import Path
import os

from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

# .env íŒŒì¼ ë¡œë“œ (í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë˜ëŠ” backend ë””ë ‰í† ë¦¬ì— ìˆì„ ìˆ˜ ìˆìŒ)
env_path = Path(__file__).parent.parent.parent / ".env"  # í”„ë¡œì íŠ¸ ë£¨íŠ¸
if not env_path.exists():
    env_path = Path(__file__).parent.parent / ".env"  # backend ë””ë ‰í† ë¦¬
if env_path.exists():
    load_dotenv(env_path)
    print(f"[Config] .env íŒŒì¼ ë¡œë“œë¨: {env_path}")

from app.services.keyword_extractor import extract_keywords
from app.services.emotion_classifier import classify_emotion
from app.services.poem_generator import generate_poem_from_keywords
from app.services.poem_model_loader import _load_poem_model

# í•™ìŠµëœ ëª¨ë¸ ì‚¬ìš©ì„ ìœ„í•œ import
import sys
from pathlib import Path
backend_path = Path(__file__).parent.parent
sys.path.insert(0, str(backend_path))
try:
    from use_trained_model import load_trained_model, generate_poem_from_prose
    HAS_TRAINED_MODEL = True
except ImportError:
    HAS_TRAINED_MODEL = False
    print("âš ï¸ í•™ìŠµëœ ëª¨ë¸ ëª¨ë“ˆì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

app = FastAPI(title="Poem API (SOLAR Instruct, Colab GPU)")

# í„°ë„/í”„ë¡ íŠ¸ ê°œë°œ í™˜ê²½ ë‹¤ì–‘ì„±ì„ ìœ„í•´ CORSëŠ” ì™€ì¼ë“œì¹´ë“œ í—ˆìš©
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],           # í•„ìš” ì‹œ íŠ¹ì • ë„ë©”ì¸ìœ¼ë¡œ ì¢íˆì„¸ìš”
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.on_event("startup")
async def startup_event():
    """
    ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ì„ ë¯¸ë¦¬ ë¡œë“œí•©ë‹ˆë‹¤.
    ì²« ìš”ì²­ ì‹œ ì§€ì—° ì‹œê°„ì„ ì¤„ì´ê¸° ìœ„í•´ ì‚¬ì „ ë¡œë”©í•©ë‹ˆë‹¤.
    """
    print("\n" + "="*80)
    print("ğŸš€ ì„œë²„ ì‹œì‘ ì¤‘: ëª¨ë¸ ì‚¬ì „ ë¡œë”© ì‹œì‘...")
    print("="*80)
    
    try:
        # ëª¨ë¸ ë¡œë”© (ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰)
        import concurrent.futures
        loop = asyncio.get_event_loop()
        with concurrent.futures.ThreadPoolExecutor() as executor:
            await loop.run_in_executor(executor, _load_poem_model)
        print("="*80)
        print("âœ… ëª¨ë¸ ì‚¬ì „ ë¡œë”© ì™„ë£Œ! ì²« ìš”ì²­ë¶€í„° ë¹ ë¥´ê²Œ ì‘ë‹µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        print("="*80 + "\n")
    except Exception as e:
        print(f"âš ï¸ ëª¨ë¸ ì‚¬ì „ ë¡œë”© ì‹¤íŒ¨: {e}")
        print("   (ì²« ìš”ì²­ ì‹œ ìë™ìœ¼ë¡œ ë¡œë“œë©ë‹ˆë‹¤.)\n")
        import traceback
        traceback.print_exc()

class PoemRequest(BaseModel):
    text: str
    lines: Optional[int] = None  # ì¤„ ìˆ˜ (í–‰)
    mood: Optional[str] = None  # ë¶„ìœ„ê¸° (ì”ì”/ë‹´ë‹´/ì“¸ì“¸)
    required_keywords: Optional[List[str]] = None  # í•„ìˆ˜ í‚¤ì›Œë“œ
    banned_words: Optional[List[str]] = None  # ê¸ˆì¹™ì–´
    use_rhyme: Optional[bool] = False  # ë‘ìš´/ë‘í–‰ë‘ìš´ ìš´ìœ¨ ì‚¬ìš© ì—¬ë¶€
    acrostic: Optional[str] = None  # ì•„í¬ë¡œìŠ¤í‹± (ì˜ˆ: "ì‚¬ë‘í•´")
    model_type: Optional[str] = None  # ëª¨ë¸ íƒ€ì…: "solar" (GPU) ë˜ëŠ” "kogpt2" (CPU)
    use_trained_model: Optional[bool] = False  # í•™ìŠµëœ ëª¨ë¸ ì‚¬ìš© ì—¬ë¶€
    trained_model_path: Optional[str] = None  # í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ (Noneì´ë©´ ìë™ ê²€ìƒ‰)
    use_gemini_improvement: Optional[bool] = True  # Gemini APIë¡œ ì‹œ ê°œì„  ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)

class PoemResponse(BaseModel):
    keywords: List[str]
    emotion: str
    emotion_confidence: float
    poem: str
    success: bool
    message: Optional[str] = None

class EmotionAnalysisRequest(BaseModel):
    poems: List[Dict]  # ì‹œ ëª©ë¡ (emotion, createdAt ë“± í¬í•¨)

class EmotionAnalysisResponse(BaseModel):
    story: str  # ê·€ì—¬ìš´ ê°ì • ì¶”ì´ ìŠ¤í† ë¦¬
    summary: str  # ê°ì • ìš”ì•½
    emoji: str  # ëŒ€í‘œ ì´ëª¨ì§€
    message: str  # ê·€ì—¬ìš´ ë©”ì‹œì§€
    success: bool

@app.get("/health")
def health():
    from app.services.poem_config import MODEL_TYPE, GEN_MODEL_ID
    from app.services.poem_model_loader import _is_gpu, _device_info
    
    device_info = _device_info()
    is_gpu = _is_gpu()
    
    model_display = f"{MODEL_TYPE.upper()}" + (f" (GPU: {device_info})" if is_gpu else " (CPU)")
    
    return {
        "ok": True,
        "service": "poem",
        "model_type": MODEL_TYPE,
        "model_id": GEN_MODEL_ID,
        "device": device_info,
        "has_gpu": is_gpu,
        "model": model_display
    }

@app.post("/api/poem/generate", response_model=PoemResponse)
async def generate_poem_from_text(request: PoemRequest):
    """
    ì‚¬ìš©ìì˜ ì¼ìƒê¸€ì„ ë°›ì•„ í‚¤ì›Œë“œ, ê°ì •ì„ ì¶”ì¶œí•˜ê³  ì‹œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    - í‚¤ì›Œë“œ: TF-IDF
    - ê°ì •: XNLI ì œë¡œìƒ· (ê¸ì •/ì¤‘ë¦½/ë¶€ì • â†’ ë¶„ìœ„ê¸° ë§¤í•‘)
    - ì‹œ: SOLAR-10.7B-Instruct (4bit, chat í…œí”Œë¦¿)
    """
    t0 = time.time()
    print("\n" + "="*80)
    print("[API] /api/poem/generate ì§„ì…")

    # ìš”ì²­ ê²€ì¦
    if not request.text or not request.text.strip():
        raise HTTPException(status_code=400, detail="í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
    text = request.text.strip()
    print(f"[API] ì…ë ¥ ê¸¸ì´: {len(text)}ì")

    # 1) í‚¤ì›Œë“œ ì¶”ì¶œ (ì‹œ ìƒì„±ê³¼ ë…ë¦½ì ìœ¼ë¡œ ì§„í–‰)
    print("[API] 1ë‹¨ê³„: í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œì‘...")
    keywords = extract_keywords(text, max_keywords=10)  # ë” ë§ì€ í‚¤ì›Œë“œ ì¶”ì¶œ
    print(f"[API] âœ“ í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ: {keywords}")
    print("=" * 60)
    print("ğŸ“ ì¶”ì¶œëœ í‚¤ì›Œë“œ:", keywords)
    print("=" * 60)

    # 2) ê°ì • ë¶„ë¥˜ (ì‹œ ìƒì„±ê³¼ ë…ë¦½ì ìœ¼ë¡œ ì§„í–‰, ì‚¬ìš©ìê°€ ë¶„ìœ„ê¸°ë¥¼ ì§€ì •í•˜ì§€ ì•Šì€ ê²½ìš°ì—ë§Œ)
    print("[API] 2ë‹¨ê³„: ê°ì • ë¶„ë¥˜ ì‹œì‘...")
    emo = classify_emotion(text)
    emotion = emo.get("emotion", "ì¤‘ë¦½")
    default_mood = emo.get("mood", "ë‹´ë‹´í•œ")
    confidence = float(emo.get("confidence", 0.0))
    
    # ì‚¬ìš©ìê°€ ì§€ì •í•œ ë¶„ìœ„ê¸°ê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ìë™ ë¶„ì„ ê²°ê³¼ ì‚¬ìš©
    mood = request.mood if request.mood else default_mood
    lines = request.lines if request.lines else 4
    
    print(f"[API] âœ“ ê°ì • ë¶„ë¥˜ ì™„ë£Œ: ê°ì •={emotion}, ë¶„ìœ„ê¸°={mood}, ì‹ ë¢°ë„={confidence:.3f}")
    print("=" * 60)
    print(f"ğŸ’­ ê°ì • ë¶„ì„ ê²°ê³¼:")
    print(f"   - ê°ì •: {emotion}")
    print(f"   - ë¶„ìœ„ê¸°: {mood} (ì‚¬ìš©ì ì§€ì •: {request.mood is not None})")
    print(f"   - ì‹ ë¢°ë„: {confidence:.3f}")
    print(f"   - ì¤„ ìˆ˜: {lines}")
    if request.required_keywords:
        print(f"   - í•„ìˆ˜ í‚¤ì›Œë“œ: {request.required_keywords}")
    if request.banned_words:
        print(f"   - ê¸ˆì¹™ì–´: {request.banned_words}")
    if request.use_rhyme:
        print(f"   - ìš´ìœ¨ ì‚¬ìš©: ì˜ˆ")
    if request.acrostic:
        print(f"   - ì•„í¬ë¡œìŠ¤í‹±: {request.acrostic}")
    print("=" * 60)

    # í•„ìˆ˜ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
    final_keywords = keywords.copy()
    if request.required_keywords:
        for kw in request.required_keywords:
            if kw not in final_keywords:
                final_keywords.insert(0, kw)  # í•„ìˆ˜ í‚¤ì›Œë“œë¥¼ ì•ì— ì¶”ê°€

    # 3) ì‹œ ìƒì„± (ìŠ¤ë ˆë“œ ì‹¤í–‰ + íƒ€ì„ì•„ì›ƒ)
    print("[API] 3ë‹¨ê³„: ì‹œ ìƒì„± ì‹œì‘...", flush=True)
    
    # í•™ìŠµëœ ëª¨ë¸ ì‚¬ìš© ì—¬ë¶€ í™•ì¸
    use_trained = request.use_trained_model and HAS_TRAINED_MODEL
    
    if use_trained:
        print("[API] í•™ìŠµëœ ëª¨ë¸ ì‚¬ìš© ëª¨ë“œ", flush=True)
        # í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ ì°¾ê¸°
        trained_model_path = request.trained_model_path
        if not trained_model_path:
            # ìë™ìœ¼ë¡œ trained_models í´ë”ì—ì„œ ì°¾ê¸°
            backend_path = Path(__file__).parent.parent
            trained_models_dir = backend_path / "trained_models"
            if trained_models_dir.exists():
                # 20251109_08ë¡œ ì‹œì‘í•˜ëŠ” ëª¨ë¸ ì°¾ê¸°
                model_folders = [f for f in trained_models_dir.iterdir() 
                                if f.is_dir() and "20251109_08" in f.name and "kogpt2" in f.name.lower()]
                if model_folders:
                    # ê°€ì¥ ìµœì‹  ëª¨ë¸ ì„ íƒ (ì´ë¦„ìœ¼ë¡œ ì •ë ¬)
                    trained_model_path = str(sorted(model_folders, key=lambda x: x.name, reverse=True)[0])
                    print(f"[API] ìë™ìœ¼ë¡œ í•™ìŠµëœ ëª¨ë¸ ì°¾ìŒ: {trained_model_path}", flush=True)
                else:
                    print("[API] âš ï¸ í•™ìŠµëœ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©", flush=True)
                    use_trained = False
            else:
                print("[API] âš ï¸ trained_models í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©", flush=True)
                use_trained = False
    
    loop = asyncio.get_event_loop()
    try:
        with concurrent.futures.ThreadPoolExecutor() as executor:
            if use_trained and trained_model_path:
                # í•™ìŠµëœ ëª¨ë¸ ì‚¬ìš©
                print(f"[API] í•™ìŠµëœ ëª¨ë¸ë¡œ ì‹œ ìƒì„± ì¤‘... (ê²½ë¡œ: {trained_model_path})", flush=True)
                
                def generate_with_trained_model():
                    # ëª¨ë¸ ë¡œë“œ (ìºì‹œ ê°€ëŠ¥í•˜ë„ë¡ ì „ì—­ ë³€ìˆ˜ ì‚¬ìš©)
                    if not hasattr(generate_with_trained_model, '_tokenizer') or \
                       not hasattr(generate_with_trained_model, '_model') or \
                       not hasattr(generate_with_trained_model, '_device') or \
                       getattr(generate_with_trained_model, '_model_path', None) != trained_model_path:
                        print(f"[API] í•™ìŠµëœ ëª¨ë¸ ë¡œë”© ì¤‘...", flush=True)
                        tokenizer, model, device = load_trained_model(trained_model_path)
                        generate_with_trained_model._tokenizer = tokenizer
                        generate_with_trained_model._model = model
                        generate_with_trained_model._device = device
                        generate_with_trained_model._model_path = trained_model_path
                        print(f"[API] í•™ìŠµëœ ëª¨ë¸ ë¡œë”© ì™„ë£Œ", flush=True)
                    
                    # í•™ìŠµëœ ëª¨ë¸ë¡œ ì‹œ ìƒì„± (ì‚°ë¬¸ì„ ì§ì ‘ ì…ë ¥)
                    raw_poem = generate_poem_from_prose(
                        text,  # ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ì‚°ë¬¸ìœ¼ë¡œ ì‚¬ìš©
                        generate_with_trained_model._tokenizer,
                        generate_with_trained_model._model,
                        generate_with_trained_model._device,
                        max_new_tokens=150  # ì‹œ ê¸¸ì´ ì¡°ì ˆ: 100 â†’ 150 (ì ë‹¹í•œ ê¸¸ì´ì˜ ì‹œ ìƒì„±)
                    )
                    
                    # Gemini APIë¡œ ì‹œ ê°œì„  (í”„ë¡¬í”„íŠ¸ ì˜µì…˜ì´ ì„¤ì •ëœ ê²½ìš°ì—ë§Œ)
                    # í”„ë¡¬í”„íŠ¸ ì˜µì…˜: lines, mood, required_keywords, banned_words, use_rhyme, acrostic
                    # í”„ë¡¬í”„íŠ¸ ì˜µì…˜ì´ ì‹¤ì œë¡œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸
                    has_prompt_options = (
                        (request.lines is not None and request.lines != 4) or  # ê¸°ë³¸ê°’ 4ê°€ ì•„ë‹Œ ê²½ìš°ë§Œ
                        (request.mood is not None and request.mood.strip() != '') or  # ë¶„ìœ„ê¸° ì„¤ì •
                        (request.required_keywords is not None and len(request.required_keywords) > 0) or  # í•„ìˆ˜ í‚¤ì›Œë“œ
                        (request.banned_words is not None and len(request.banned_words) > 0) or  # ê¸ˆì¹™ì–´
                        (request.use_rhyme is True) or  # ìš´ìœ¨ ì‚¬ìš©
                        (request.acrostic is not None and request.acrostic.strip() != '')  # ì•„í¬ë¡œìŠ¤í‹±
                    )
                    
                    print(f"[API] í”„ë¡¬í”„íŠ¸ ì˜µì…˜ ì²´í¬: lines={request.lines}, mood={request.mood}, required_keywords={request.required_keywords}, banned_words={request.banned_words}, use_rhyme={request.use_rhyme}, acrostic={request.acrostic}", flush=True)
                    print(f"[API] has_prompt_options={has_prompt_options}, use_gemini_improvement={request.use_gemini_improvement}", flush=True)
                    
                    # í”„ë¡¬í”„íŠ¸ ì˜µì…˜ì´ ìˆê³ , use_gemini_improvementê°€ Falseê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ê°œì„ 
                    if has_prompt_options and request.use_gemini_improvement is not False:
                        try:
                            print(f"[API] í”„ë¡¬í”„íŠ¸ ì˜µì…˜ ì ìš©ë¨ â†’ Geminië¡œ ì‹œ ê°œì„  ì‹œì‘", flush=True)
                            improved_poem = improve_poem_with_gemini(raw_poem, text)
                            if improved_poem and improved_poem != raw_poem:
                                print(f"[API] âœ“ Gemini ê°œì„  ì™„ë£Œ: ì›ë³¸ {len(raw_poem)}ì â†’ ê°œì„  {len(improved_poem)}ì", flush=True)
                                return improved_poem
                            else:
                                print(f"[API] âš ï¸ Gemini ê°œì„  ê²°ê³¼ê°€ ì›ë³¸ê³¼ ë™ì¼í•˜ê±°ë‚˜ ë¹„ì–´ìˆìŒ. ì›ë³¸ ë°˜í™˜", flush=True)
                                return raw_poem
                        except Exception as e:
                            print(f"[API] âŒ Gemini ì‹œ ê°œì„  ì‹¤íŒ¨, ì›ë³¸ ì‚¬ìš©: {e}", flush=True)
                            import traceback
                            traceback.print_exc()
                            return raw_poem
                    else:
                        if has_prompt_options:
                            print(f"[API] í”„ë¡¬í”„íŠ¸ ì˜µì…˜ ì ìš©ë¨, í•˜ì§€ë§Œ Gemini ê°œì„  ë¹„í™œì„±í™” (use_gemini_improvement={request.use_gemini_improvement})", flush=True)
                        else:
                            print(f"[API] í”„ë¡¬í”„íŠ¸ ì˜µì…˜ ì—†ìŒ â†’ Gemini ê°œì„  ìƒëµ (ì›ë³¸ ì‹œ ë°˜í™˜)", flush=True)
                        return raw_poem
                
                poem = await asyncio.wait_for(
                    loop.run_in_executor(executor, generate_with_trained_model),
                    timeout=300.0
                )
            else:
                # ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©
                print("[API] ê¸°ë³¸ ëª¨ë¸ë¡œ ì‹œ ìƒì„± ì¤‘... (ì†ë„ ìµœì í™”: 80í† í°)", flush=True)
                poem = await asyncio.wait_for(
                    loop.run_in_executor(
                        executor, 
                        generate_poem_from_keywords, 
                        final_keywords, 
                        mood, 
                        lines, 
                        80, 
                        text,
                        request.banned_words,
                        request.use_rhyme,
                        request.acrostic,
                        request.model_type  # ëª¨ë¸ íƒ€ì… ì „ë‹¬
                    ),
                    timeout=300.0  # 5ë¶„ íƒ€ì„ì•„ì›ƒ (ì²« ìš”ì²­ ì‹œ ëª¨ë¸ ë¡œë”© + ìƒì„± + ë²ˆì—­ ì‹œê°„ í¬í•¨)
                )
        print(f"[API] âœ“ ì‹œ ìƒì„± ì™„ë£Œ (ê¸¸ì´ {len(poem)}ì)", flush=True)
    except asyncio.TimeoutError:
        print("[API] âŒ íƒ€ì„ì•„ì›ƒ(>300s)", flush=True)
        raise HTTPException(status_code=504, detail="ì‹œ ìƒì„± ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤ (5ë¶„). ì²« ìš”ì²­ì€ ëª¨ë¸ ë¡œë”©ìœ¼ë¡œ ë” ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.")
    except Exception as e:
        error_type = type(e).__name__
        msg = str(e) or "ì‹œ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        print(f"[API] âŒ ìƒì„± ì˜ˆì™¸: {error_type}: {msg}")
        import traceback
        print("[API] ì „ì²´ íŠ¸ë ˆì´ìŠ¤ë°±:")
        traceback.print_exc()
        
        # ë” êµ¬ì²´ì ì¸ ì—ëŸ¬ ë©”ì‹œì§€ ì œê³µ
        if "ë©”ëª¨ë¦¬" in msg or "memory" in msg.lower() or "cuda" in msg.lower():
            detail_msg = f"GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ë˜ëŠ” CUDA ì˜¤ë¥˜ì…ë‹ˆë‹¤. {msg[:200]}"
        elif "ìƒì„±í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤" in msg or "ë¹„ì–´ìˆìŠµë‹ˆë‹¤" in msg:
            detail_msg = f"ëª¨ë¸ì´ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. {msg[:200]}"
        else:
            detail_msg = f"ì‹œ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {msg[:200]}"
        
        raise HTTPException(status_code=500, detail=detail_msg)

    # 4) ê²€ì¦(ì•„ì£¼ ê´€ëŒ€)
    poem_clean = (poem or "").strip()
    if not poem_clean:
        print("[API] âŒ ìµœì¢… ê²°ê³¼ ë¹ˆ ë¬¸ìì—´")
        raise HTTPException(status_code=500, detail="ì‹œ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ìƒì„±ëœ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")

    # í•œê¸€ ë¬¸ìê°€ 3ì ì´ìƒì´ë©´ í†µê³¼
    korean_chars = sum(1 for c in poem_clean if ord('ê°€') <= ord(c) <= ord('í£'))
    print(f"[API] ìµœì¢… ê²€ì¦: ê¸¸ì´={len(poem_clean)}ì, í•œê¸€ë¬¸ì={korean_chars}ì")
    if korean_chars < 3 and len(poem_clean) < 3:
        raise HTTPException(status_code=500, detail="ì‹œ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ìƒì„±ëœ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤.")

    print(f"[API] ì „ì²´ ì²˜ë¦¬ ì‹œê°„: {time.time() - t0:.2f}s")
    print("="*80)

    return PoemResponse(
        keywords=keywords,
        emotion=emotion,
        emotion_confidence=confidence,
        poem=poem_clean,
        success=True,
        message="ì‹œê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.",
    )

def improve_poem_with_gemini(raw_poem: str, original_prose: str = "") -> str:
    """
    Gemini APIë¥¼ ì‚¬ìš©í•˜ì—¬ koGPT2ë¡œ ìƒì„±í•œ ì‹œë¥¼ ê°œì„ í•©ë‹ˆë‹¤.
    - ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì œê±° (ë‰´ìŠ¤ ìŠ¤íƒ€ì¼ ë¬¸ì¥ ë“±)
    - ì‚°ë¬¸ í•„í„°ë§
    - ì¤„ë°”ê¿ˆ ê°œì„ 
    - ì‹œì  í‘œí˜„ ê°œì„ 
    """
    try:
        import google.generativeai as genai
        
        # API í‚¤ í™•ì¸
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            print("[Gemini] âš ï¸ GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì›ë³¸ ì‹œ ë°˜í™˜")
            return raw_poem
        
        genai.configure(api_key=api_key)
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì°¾ê¸°
        try:
            print("[Gemini] ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ í™•ì¸ ì¤‘...", flush=True)
            available_models = []
            for model_info in genai.list_models():
                if 'generateContent' in model_info.supported_generation_methods:
                    available_models.append(model_info.name)
            
            print(f"[Gemini] ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ {len(available_models)}ê°œ ë°œê²¬", flush=True)
            
            preferred_models = [
                'models/gemini-2.5-flash',
                'models/gemini-2.5-flash-lite-preview-06-17',
                'models/gemini-1.5-flash',
                'models/gemini-1.5-pro',
                'models/gemini-pro',
            ]
            
            model = None
            selected_model_name = None
            for model_name in preferred_models:
                if model_name in available_models:
                    try:
                        model = genai.GenerativeModel(model_name)
                        selected_model_name = model_name
                        print(f"[Gemini] ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_name}", flush=True)
                        break
                    except Exception as e:
                        print(f"[Gemini] ëª¨ë¸ {model_name} ë¡œë“œ ì‹¤íŒ¨: {e}", flush=True)
                        continue
            
            if model is None and available_models:
                selected_model_name = available_models[0]
                model = genai.GenerativeModel(selected_model_name)
                print(f"[Gemini] ëŒ€ì²´ ëª¨ë¸ ì‚¬ìš©: {selected_model_name}", flush=True)
            elif model is None:
                print("[Gemini] ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì›ë³¸ ì‹œ ë°˜í™˜", flush=True)
                return raw_poem
        except Exception as e:
            print(f"[Gemini] ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}, ì›ë³¸ ì‹œ ë°˜í™˜", flush=True)
            import traceback
            traceback.print_exc()
            return raw_poem
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„± (ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì œê±° ë° ì‹œì  í‘œí˜„ ê°œì„ )
        prompt = f"""ë‹¤ìŒì€ AIê°€ ìƒì„±í•œ í•œêµ­ì–´ ì‹œì…ë‹ˆë‹¤. ì´ ì‹œë¥¼ ê°œì„ í•´ì£¼ì„¸ìš”.

ì›ë³¸ ì‚°ë¬¸ (ì°¸ê³ ìš©):
{original_prose[:200] if original_prose else "ì—†ìŒ"}

ìƒì„±ëœ ì‹œ:
{raw_poem}

ë‹¤ìŒ ì‘ì—…ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”:
1. ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì œê±° (ë‰´ìŠ¤ ìŠ¤íƒ€ì¼ ë¬¸ì¥, ì„¤ëª…ë¬¸ ë“±)
2. "ì‹œ:", "ì‚°ë¬¸:" ê°™ì€ í”„ë¡¬í”„íŠ¸ íŒ¨í„´ ì œê±°
3. ì ì ˆí•œ ì¤„ë°”ê¿ˆ ìœ ì§€ (ë¬¸ì¥ ëê³¼ ì‰¼í‘œ ë’¤)
4. ì‹œì  í‘œí˜„ ê°œì„  (ìì—°ìŠ¤ëŸ½ê³  ì•„ë¦„ë‹¤ìš´ í‘œí˜„ìœ¼ë¡œ ë‹¤ë“¬ê¸°)

ì¤‘ìš”: ì‹œì˜ ì£¼ì œì™€ í•µì‹¬ ì˜ë¯¸ëŠ” ìœ ì§€í•˜ë˜, í‘œí˜„ì„ ë” ì‹œë‹µê²Œ ê°œì„ í•´ì£¼ì„¸ìš”.

ê°œì„ ëœ ì‹œë§Œ ì¶œë ¥í•´ì£¼ì„¸ìš”. ì„¤ëª…ì´ë‚˜ ì¶”ê°€ í…ìŠ¤íŠ¸ ì—†ì´ ì‹œë§Œ ì¶œë ¥í•˜ì„¸ìš”.
"""
        
        print(f"[Gemini] í”„ë¡¬í”„íŠ¸ ì „ì†¡ ì¤‘... (ì›ë³¸ ì‹œ ê¸¸ì´: {len(raw_poem)}ì)", flush=True)
        response = model.generate_content(prompt)
        
        # ì‘ë‹µ íŒŒì‹± (ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›)
        improved_poem = ""
        if hasattr(response, 'text'):
            improved_poem = response.text.strip()
        elif hasattr(response, 'candidates') and len(response.candidates) > 0:
            if hasattr(response.candidates[0], 'content'):
                if hasattr(response.candidates[0].content, 'parts'):
                    improved_poem = ''.join([part.text for part in response.candidates[0].content.parts if hasattr(part, 'text')]).strip()
        
        print(f"[Gemini] ì‘ë‹µ ë°›ìŒ: {len(improved_poem)}ì", flush=True)
        if improved_poem:
            print(f"[Gemini] ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 100ì): {improved_poem[:100]}", flush=True)
        
        # ê²°ê³¼ ê²€ì¦
        if improved_poem and len(improved_poem) > 10:
            print(f"[Gemini] ì‹œ ê°œì„  ì™„ë£Œ: {len(raw_poem)}ì â†’ {len(improved_poem)}ì", flush=True)
            return improved_poem
        else:
            print(f"[Gemini] ê°œì„  ê²°ê³¼ê°€ ë¹„ì–´ìˆê±°ë‚˜ ë„ˆë¬´ ì§§ìŒ ({len(improved_poem)}ì). ì›ë³¸ ì‹œ ë°˜í™˜", flush=True)
            return raw_poem
        
    except Exception as e:
        print(f"[Gemini] ì‹œ ê°œì„  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return raw_poem  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì›ë³¸ ë°˜í™˜

def analyze_emotions_cutely(poems: List[Dict]) -> Dict[str, str]:
    """
    Gemini APIë¥¼ ì‚¬ìš©í•˜ì—¬ ê°ì • ë°ì´í„°ë¥¼ ê·€ì—¬ìš´ ìŠ¤í† ë¦¬ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    try:
        import google.generativeai as genai
        
        # API í‚¤ í™•ì¸
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            print("[Gemini] âš ï¸ GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return {
                "story": "ê°ì • ë¶„ì„ì„ ìœ„í•´ Gemini API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.",
                "summary": "API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.",
                "emoji": "ğŸ”‘",
                "message": "ì„¤ì •ì—ì„œ API í‚¤ë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”.",
                "success": False
            }
        
        genai.configure(api_key=api_key)
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ í™•ì¸ ë° ì ì ˆí•œ ëª¨ë¸ ì„ íƒ
        try:
            # ë¨¼ì € ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ í™•ì¸
            available_models = []
            for model_info in genai.list_models():
                if 'generateContent' in model_info.supported_generation_methods:
                    available_models.append(model_info.name)
            
            print(f"[Gemini] ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {available_models[:5]}")  # ì²˜ìŒ 5ê°œë§Œ ì¶œë ¥
            
            # ìš°ì„ ìˆœìœ„: ì•ˆì •ì ì¸ ëª¨ë¸ë¶€í„° ì‹œë„
            preferred_models = [
                'models/gemini-2.5-flash',  # ê°€ì¥ ì•ˆì •ì ì¸ ìµœì‹  ëª¨ë¸
                'models/gemini-2.5-flash-lite-preview-06-17',
                'models/gemini-1.5-flash',
                'models/gemini-1.5-pro',
                'models/gemini-pro',
            ]
            
            model = None
            # ìš°ì„ ìˆœìœ„ ëª¨ë¸ ì‹œë„
            for model_name in preferred_models:
                if model_name in available_models:
                    try:
                        model = genai.GenerativeModel(model_name)
                        print(f"[Gemini] ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_name}")
                        break
                    except Exception as e:
                        print(f"[Gemini] ëª¨ë¸ {model_name} ì‹œë„ ì‹¤íŒ¨: {e}")
                        continue
            
            # ìš°ì„ ìˆœìœ„ ëª¨ë¸ì´ ëª¨ë‘ ì‹¤íŒ¨í•˜ë©´ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì¤‘ ì²« ë²ˆì§¸ ì‚¬ìš©
            if model is None:
                if available_models:
                    # 'models/gemini-xxx' í˜•ì‹ì—ì„œ 'gemini-xxx'ë§Œ ì¶”ì¶œ
                    first_model = available_models[0]
                    print(f"[Gemini] ì‚¬ìš© ê°€ëŠ¥í•œ ì²« ë²ˆì§¸ ëª¨ë¸ ì‚¬ìš©: {first_model}")
                    model = genai.GenerativeModel(first_model)
                else:
                    raise Exception("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    
        except Exception as e:
            print(f"[Gemini] ëª¨ë¸ ëª©ë¡ í™•ì¸ ì‹¤íŒ¨, ê¸°ë³¸ ëª¨ë¸ ì‹œë„: {e}")
            # ê¸°ë³¸ ëª¨ë¸ ì‹œë„ (fallback)
            try:
                model = genai.GenerativeModel('models/gemini-2.5-flash')
            except:
                try:
                    model = genai.GenerativeModel('models/gemini-1.5-flash')
                except:
                    raise Exception(f"ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì˜¤ë¥˜: {e}")
        
        # ê°ì • ë°ì´í„° ì •ë¦¬
        emotion_data = {}
        for poem in poems:
            if not poem.get('emotion') or not poem.get('createdAt'):
                continue
            emotion = poem['emotion']
            date = poem['createdAt'][:10]  # YYYY-MM-DD
            
            if date not in emotion_data:
                emotion_data[date] = {}
            emotion_data[date][emotion] = emotion_data[date].get(emotion, 0) + 1
        
        # ê°ì •ë³„ ì´ ê°œìˆ˜
        emotion_counts = {}
        for poem in poems:
            if poem.get('emotion'):
                emotion = poem['emotion']
                emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = f"""ë‹¹ì‹ ì€ ê°ì • ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ê°ì • ë°ì´í„°ë¥¼ ìì—°ìŠ¤ëŸ½ê³  ë”°ëœ»í•œ í†¤ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”.

ê°ì • ë°ì´í„°:
{emotion_data}

ê°ì •ë³„ ê°œìˆ˜:
{emotion_counts}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:

1. ê°ì • ì¶”ì´ ìŠ¤í† ë¦¬ (100-150ì): ë‚ ì§œë³„ ê°ì • ë³€í™”ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”. ì˜ˆ: "ì´ë²ˆ ì£¼ëŠ” ê¸°ì¨ì´ ë§ì´ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. ì›”ìš”ì¼ë¶€í„° ê¸°ì¨ì´ ì¦ê°€í•˜ê¸° ì‹œì‘í–ˆê³ ..."
2. ê°ì • ìš”ì•½ (50-80ì): ì „ì²´ì ì¸ ê°ì • íŒ¨í„´ì„ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”.
3. ëŒ€í‘œ ì´ëª¨ì§€: ê°€ì¥ ë§ì´ ë‚˜íƒ€ë‚œ ê°ì •ì— ë§ëŠ” ì´ëª¨ì§€ í•˜ë‚˜
4. ë”°ëœ»í•œ ë©”ì‹œì§€ (30-50ì): ì‚¬ìš©ìì—ê²Œ ì „í•˜ëŠ” ìì—°ìŠ¤ëŸ¬ìš´ ë©”ì‹œì§€

ì£¼ì˜ì‚¬í•­:
- ì–´ë¦°ì•„ì´ì—ê²Œ í•˜ëŠ” ë§íˆ¬ë‚˜ ê³¼ë„í•˜ê²Œ ê·€ì—¬ìš´ í‘œí˜„ì€ í”¼í•´ì£¼ì„¸ìš”
- ìì—°ìŠ¤ëŸ½ê³  ì„±ìˆ™í•œ í†¤ì„ ìœ ì§€í•´ì£¼ì„¸ìš”
- ë”°ëœ»í•˜ì§€ë§Œ ì „ë¬¸ì ì¸ ëŠë‚Œì„ ì£¼ì„¸ìš”

í˜•ì‹:
ìŠ¤í† ë¦¬: [ì—¬ê¸°ì— ìŠ¤í† ë¦¬]
ìš”ì•½: [ì—¬ê¸°ì— ìš”ì•½]
ì´ëª¨ì§€: [ì—¬ê¸°ì— ì´ëª¨ì§€]
ë©”ì‹œì§€: [ì—¬ê¸°ì— ë©”ì‹œì§€]
"""
        
        response = model.generate_content(prompt)
        result_text = response.text
        
        # ì‘ë‹µ íŒŒì‹±
        story = ""
        summary = ""
        emoji = "ğŸ’­"
        message = ""
        
        lines = result_text.split('\n')
        for line in lines:
            if line.startswith('ìŠ¤í† ë¦¬:'):
                story = line.replace('ìŠ¤í† ë¦¬:', '').strip()
            elif line.startswith('ìš”ì•½:'):
                summary = line.replace('ìš”ì•½:', '').strip()
            elif line.startswith('ì´ëª¨ì§€:'):
                emoji = line.replace('ì´ëª¨ì§€:', '').strip()
            elif line.startswith('ë©”ì‹œì§€:'):
                message = line.replace('ë©”ì‹œì§€:', '').strip()
        
        # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’
        if not story:
            story = result_text[:150] if result_text else "ê°ì • ë°ì´í„°ë¥¼ ë¶„ì„í–ˆì–´ìš”!"
        if not summary:
            summary = "ê°ì • ë³€í™”ë¥¼ í™•ì¸í–ˆì–´ìš”."
        if not message:
            message = "ì˜¤ëŠ˜ë„ ìˆ˜ê³ í•˜ì…¨ì–´ìš”!"
        
        return {
            "story": story,
            "summary": summary,
            "emoji": emoji,
            "message": message,
            "success": True
        }
        
    except Exception as e:
        print(f"[Gemini] âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return {
            "story": "ê°ì • ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
            "summary": "ì˜¤ë¥˜ ë°œìƒ",
            "emoji": "ğŸ˜¢",
            "message": "ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
            "success": False
        }

@app.post("/api/emotion/analyze-cute", response_model=EmotionAnalysisResponse)
async def analyze_emotions_cutely_endpoint(request: EmotionAnalysisRequest):
    """
    ê°ì • ë°ì´í„°ë¥¼ ë°›ì•„ì„œ Gemini APIë¡œ ê·€ì—¬ìš´ ìŠ¤í† ë¦¬ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    print("\n" + "="*80)
    print("[API] /api/emotion/analyze-cute ì§„ì…")
    print(f"[API] ì‹œ ê°œìˆ˜: {len(request.poems)}ê°œ")
    
    if not request.poems:
        raise HTTPException(status_code=400, detail="ì‹œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    result = analyze_emotions_cutely(request.poems)
    
    print(f"[API] âœ“ ê°ì • ë¶„ì„ ì™„ë£Œ")
    print("="*80)
    
    return EmotionAnalysisResponse(**result)