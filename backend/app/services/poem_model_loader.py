# -*- coding: utf-8 -*-
"""
ì‹œ ìƒì„± ëª¨ë¸ ë¡œë”© ê´€ë ¨ í•¨ìˆ˜
"""

import time
import traceback

import torch
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
)
from typing import Tuple, Optional

from app.services.poem_config import MODEL_TYPE, GEN_MODEL_ID

# ======== ê¸€ë¡œë²Œ ìºì‹œ ========
_gen_tok: Optional[AutoTokenizer] = None
_gen_model: Optional[AutoModelForCausalLM] = None
_gen_tok_kogpt2: Optional[AutoTokenizer] = None
_gen_model_kogpt2: Optional[AutoModelForCausalLM] = None
_gen_tok_solar: Optional[AutoTokenizer] = None
_gen_model_solar: Optional[AutoModelForCausalLM] = None


def _is_gpu() -> bool:
    return torch.cuda.is_available()


def _device_info() -> str:
    try:
        if _is_gpu():
            try:
                name = torch.cuda.get_device_name(0)
                mem = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                return f"GPU(name={name}, VRAMâ‰ˆ{mem:.1f}GB)"
            except Exception as e:
                return f"GPU(unknown, error={str(e)[:50]})"
        return "CPU"
    except Exception as e:
        return f"Error: {str(e)[:50]}"


def _log_header(title: str):
    print("[_log_header] í•¨ìˆ˜ ì‹œì‘", flush=True)
    try:
        print("[_log_header] ì²« ë²ˆì§¸ print ì „", flush=True)
        print("\n" + "=" * 80, flush=True)
        print("[_log_header] ë‘ ë²ˆì§¸ print ì „", flush=True)
        print(f"[poem_generator] {title}", flush=True)
        print("[_log_header] ì„¸ ë²ˆì§¸ print ì „", flush=True)
        print("=" * 80, flush=True)
        print("[_log_header] í•¨ìˆ˜ ì™„ë£Œ", flush=True)
    except Exception as e:
        print(f"[_log_header] ì˜¤ë¥˜ ë°œìƒ: {e}", flush=True)
        import traceback
        traceback.print_exc()
        raise


def _load_poem_model(model_type: Optional[str] = None) -> Tuple[AutoTokenizer, AutoModelForCausalLM]:
    """
    ëª¨ë¸ ë¡œë”© (ìºì‹œ ì§€ì›)
    model_type: "solar" ë˜ëŠ” "kogpt2", Noneì´ë©´ ê¸°ë³¸ê°’(MODEL_TYPE) ì‚¬ìš©
    """
    global _gen_tok, _gen_model, _gen_tok_kogpt2, _gen_model_kogpt2, _gen_tok_solar, _gen_model_solar
    
    # ì‚¬ìš©í•  ëª¨ë¸ íƒ€ì… ê²°ì •
    from app.services.poem_config import MODEL_TYPE as DEFAULT_MODEL_TYPE
    target_model_type = (model_type or DEFAULT_MODEL_TYPE).lower()
    
    if target_model_type not in ["solar", "kogpt2"]:
        print(f"[_load_poem_model] âš ï¸ ì˜ëª»ëœ ëª¨ë¸ íƒ€ì…: {target_model_type}, ê¸°ë³¸ê°’ ì‚¬ìš©")
        target_model_type = DEFAULT_MODEL_TYPE
    
    # ìºì‹œ í™•ì¸
    if target_model_type == "kogpt2":
        if _gen_tok_kogpt2 is not None and _gen_model_kogpt2 is not None:
            print("[_load_poem_model] ìºì‹œëœ koGPT2 ëª¨ë¸ ì¬ì‚¬ìš©", flush=True)
            return _gen_tok_kogpt2, _gen_model_kogpt2
    else:  # solar
        if _gen_tok_solar is not None and _gen_model_solar is not None:
            print("[_load_poem_model] ìºì‹œëœ SOLAR ëª¨ë¸ ì¬ì‚¬ìš©", flush=True)
            return _gen_tok_solar, _gen_model_solar
    
    # ê¸°ì¡´ ìºì‹œ í™•ì¸ (í•˜ìœ„ í˜¸í™˜ì„±)
    if _gen_tok is not None and _gen_model is not None and target_model_type == DEFAULT_MODEL_TYPE:
        print("[_load_poem_model] ìºì‹œëœ í† í¬ë‚˜ì´ì €/ëª¨ë¸ ì¬ì‚¬ìš©", flush=True)
        return _gen_tok, _gen_model

    print("[_load_poem_model] _log_header í˜¸ì¶œ ì „", flush=True)
    try:
        _log_header("ëª¨ë¸ ë¡œë”© ì‹œì‘")
        print("[_load_poem_model] _log_header ì •ìƒ ì™„ë£Œ", flush=True)
    except Exception as e:
        print(f"[_load_poem_model] _log_header ì˜¤ë¥˜: {e}", flush=True)
        import traceback
        traceback.print_exc()
        raise
    print("[_load_poem_model] _log_header í˜¸ì¶œ í›„", flush=True)
    print("[_load_poem_model] í—¤ë” ì¶œë ¥ ì™„ë£Œ", flush=True)
    
    # ëª¨ë¸ íƒ€ì…ì— ë”°ë¼ ëª¨ë¸ ID ê²°ì •
    if target_model_type == "kogpt2":
        model_id = "skt/kogpt2-base-v2"
    else:
        model_id = "upstage/SOLAR-10.7B-Instruct-v1.0"
    
    print(f"[_load_poem_model] ëª¨ë¸ íƒ€ì…: {target_model_type}, ëª¨ë¸ ID: {model_id}", flush=True)
    print("[_load_poem_model] ë””ë°”ì´ìŠ¤ ì •ë³´ í™•ì¸ ì¤‘...", flush=True)
    try:
        device_info = _device_info()
        print(f"[_load_poem_model] ì‹¤í–‰ ë””ë°”ì´ìŠ¤: {device_info}", flush=True)
    except Exception as e:
        print(f"[_load_poem_model] âš ï¸ ë””ë°”ì´ìŠ¤ ì •ë³´ í™•ì¸ ì‹¤íŒ¨: {e}", flush=True)
        device_info = "ì•Œ ìˆ˜ ì—†ìŒ"
    
    start_all = time.time()
    print(f"[_load_poem_model] ì‹œì‘ ì‹œê°„ ê¸°ë¡ ì™„ë£Œ", flush=True)

    # 1) í† í¬ë‚˜ì´ì €
    print("[_load_poem_model] ===== 1ë‹¨ê³„: í† í¬ë‚˜ì´ì € ë¡œë”© ì‹œì‘ =====")
    t0 = time.time()
    print("[_load_poem_model] í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...")
    print(f"[_load_poem_model] ëª¨ë¸ ID: {GEN_MODEL_ID}")
    print("[_load_poem_model] AutoTokenizer.from_pretrained() í˜¸ì¶œ ì „...")
    
    try:
        print("[_load_poem_model] â³ í† í¬ë‚˜ì´ì € ë‹¤ìš´ë¡œë“œ/ë¡œë”© ì¤‘ (ì´ ê³¼ì •ì€ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)...")
        import sys
        sys.stdout.flush()  # ë²„í¼ ê°•ì œ ì¶œë ¥
        
        tok = AutoTokenizer.from_pretrained(model_id)
        print("[_load_poem_model] âœ“ AutoTokenizer.from_pretrained() í˜¸ì¶œ ì™„ë£Œ")
        sys.stdout.flush()
        
        print("[_load_poem_model] âœ“ í† í¬ë‚˜ì´ì € ê°ì²´ ìƒì„± ì™„ë£Œ")
        
        if tok.pad_token is None:
            tok.pad_token = tok.eos_token
            print("[_load_poem_model] pad_token â†’ eos_tokenìœ¼ë¡œ ì„¤ì •")
        
        print(f"[_load_poem_model] âœ“ í† í¬ë‚˜ì´ì € ë¡œë”© ì™„ë£Œ ({time.time() - t0:.2f}s)")
        print(f"[_load_poem_model] í† í¬ë‚˜ì´ì € vocab í¬ê¸°: {len(tok)}")
    except Exception as e:
        print(f"[_load_poem_model] âŒ í† í¬ë‚˜ì´ì € ë¡œë”© ì‹¤íŒ¨: {e}")
        traceback.print_exc()
        raise Exception(f"í† í¬ë‚˜ì´ì € ë¡œë”© ì‹¤íŒ¨: {str(e)[:200]}")

    # 2) ëª¨ë¸
    if target_model_type == "kogpt2":
        # koGPT2ëŠ” ì‘ì€ ëª¨ë¸ì´ë¯€ë¡œ CPUì—ì„œë„ ì‹¤í–‰ ê°€ëŠ¥ (ì–‘ìí™” ë¶ˆí•„ìš”)
        print(f"[_load_poem_model] koGPT2 ëª¨ë¸ ë¡œë”© (CPU/GPU ëª¨ë‘ ê°€ëŠ¥)")
        t1 = time.time()
        try:
            device = "cuda" if _is_gpu() else "cpu"
            print(f"[_load_poem_model] ë””ë°”ì´ìŠ¤: {device}")
            print("[_load_poem_model] ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ë¡œë”© ì‹œì‘...")
            
            model = AutoModelForCausalLM.from_pretrained(
                model_id,
                torch_dtype=torch.float32 if device == "cpu" else torch.float16,
            )
            model = model.to(device).eval()
            print("[_load_poem_model] âœ“ ëª¨ë¸ ê°ì²´ ìƒì„± ë° eval ëª¨ë“œ ì„¤ì • ì™„ë£Œ")
            print(f"[_load_poem_model] âœ“ ëª¨ë¸ ë¡œë”© ì™„ë£Œ ({time.time() - t1:.2f}s, ë””ë°”ì´ìŠ¤: {device})")
            
            # ìºì‹œì— ì €ì¥
            _gen_tok_kogpt2 = tok
            _gen_model_kogpt2 = model
            _gen_tok = tok  # í•˜ìœ„ í˜¸í™˜ì„±
            _gen_model = model
        except Exception as e:
            print(f"[_load_poem_model] âŒ koGPT2 ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            traceback.print_exc()
            raise Exception(f"koGPT2 ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {str(e)[:200]}")
    elif _is_gpu():
        print("[_load_poem_model] GPU ê°ì§€ë¨ â†’ 4bit NF4 ì–‘ìí™” + device_map=auto")
        try:
            # GPU ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸ (ë¡œë”© ì „)
            print("[_load_poem_model] GPU ì •ë³´ í™•ì¸ ì¤‘...")
            gpu_name = torch.cuda.get_device_name(0)
            gpu_mem_total = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            gpu_mem_allocated = torch.cuda.memory_allocated(0) / (1024**3)
            print(f"[_load_poem_model] âœ“ GPU ì •ë³´: {gpu_name}")
            print(f"[_load_poem_model] âœ“ GPU ë©”ëª¨ë¦¬: ì´ {gpu_mem_total:.1f}GB, ì‚¬ìš© ì¤‘ {gpu_mem_allocated:.2f}GB")
        except Exception as e:
            print(f"[_load_poem_model] âš ï¸ GPU ì •ë³´ í™•ì¸ ì‹¤íŒ¨: {e}")
        
        print("[_load_poem_model] BitsAndBytesConfig ì„¤ì • ì¤‘...")
        bnb_cfg = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16,
        )
        print("[_load_poem_model] âœ“ BitsAndBytesConfig ì„¤ì • ì™„ë£Œ")
        
        t1 = time.time()
        try:
            print("[_load_poem_model] ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ë¡œë”© ì‹œì‘...")
            print("[_load_poem_model] â³ ì´ ê³¼ì •ì€ ëª‡ ë¶„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤ (ëª¨ë¸ í¬ê¸°: ~21GB)")
            print("[_load_poem_model] â³ ì§„í–‰ ìƒí™©ì„ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...")
            
            # ëª¨ë¸ ë¡œë”© ì‹œë„
            model = AutoModelForCausalLM.from_pretrained(
                model_id,
                quantization_config=bnb_cfg,
                device_map="auto",
                low_cpu_mem_usage=True,
            )
            print("[_load_poem_model] âœ“ ëª¨ë¸ ê°ì²´ ìƒì„± ì™„ë£Œ")
            
            print("[_load_poem_model] ëª¨ë¸ì„ eval ëª¨ë“œë¡œ ì„¤ì • ì¤‘...")
            model.eval()
            print("[_load_poem_model] âœ“ eval ëª¨ë“œ ì„¤ì • ì™„ë£Œ")
            
            # ìºì‹œì— ì €ì¥
            _gen_tok_solar = tok
            _gen_model_solar = model
            _gen_tok = tok  # í•˜ìœ„ í˜¸í™˜ì„±
            _gen_model = model
            
            load_time = time.time() - t1
            print(f"[_load_poem_model] âœ“ ëª¨ë¸ ë¡œë”©/ë°°ì¹˜ ì™„ë£Œ ({load_time:.2f}s)")
            
            # GPU ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸ (ë¡œë”© í›„)
            try:
                gpu_mem_allocated = torch.cuda.memory_allocated(0) / (1024**3)
                gpu_mem_reserved = torch.cuda.memory_reserved(0) / (1024**3)
                print(f"[_load_poem_model] GPU ë©”ëª¨ë¦¬ (ë¡œë”© í›„): í• ë‹¹={gpu_mem_allocated:.2f}GB, ìºì‹œ={gpu_mem_reserved:.2f}GB")
            except:
                pass
        except RuntimeError as e:
            error_msg = str(e)
            print(f"[_load_poem_model] âŒ 4bit ë¡œë”© ì‹¤íŒ¨ (RuntimeError): {error_msg}")
            traceback.print_exc()
            if "out of memory" in error_msg.lower() or "CUDA" in error_msg:
                raise Exception(f"GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ë˜ëŠ” CUDA ì˜¤ë¥˜: ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëŸ°íƒ€ì„ì„ ì¬ì‹œì‘í•˜ê±°ë‚˜ ë” í° GPUë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. ({error_msg[:200]})")
            raise Exception(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {error_msg[:200]}")
        except Exception as e:
            error_msg = str(e)
            print(f"[_load_poem_model] âŒ 4bit ë¡œë”© ì‹¤íŒ¨: {error_msg}")
            traceback.print_exc()
            raise Exception(f"ëª¨ë¸ ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {error_msg[:200]}")
    else:
        print("[_load_poem_model] âš ï¸ GPU ì—†ìŒ â†’ CPU float32 ë¡œë“œ(ë§¤ìš° ëŠë¦¼, ê¶Œì¥ X)")
        print("[_load_poem_model] âš ï¸ CPU ëª¨ë“œëŠ” ë§¤ìš° ëŠë¦¬ë©° ë©”ëª¨ë¦¬ ë¶€ì¡±ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
        print("[_load_poem_model] ğŸ’¡ CPUë¥¼ ì‚¬ìš©í•˜ì‹œë ¤ë©´ POEM_MODEL_TYPE=kogpt2 í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”")
        t1 = time.time()
        try:
            print("[_load_poem_model] CPU ëª¨ë“œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ë¡œë”© ì‹œì‘...")
            model = AutoModelForCausalLM.from_pretrained(
                model_id,
                torch_dtype=torch.float32,
                low_cpu_mem_usage=True,
            )
            print("[_load_poem_model] âœ“ ëª¨ë¸ ê°ì²´ ìƒì„± ì™„ë£Œ")
            model = model.to("cpu").eval()
            print("[_load_poem_model] âœ“ CPUë¡œ ì´ë™ ë° eval ëª¨ë“œ ì„¤ì • ì™„ë£Œ")
            print(f"[_load_poem_model] âœ“ ëª¨ë¸ ë¡œë”© ì™„ë£Œ ({time.time() - t1:.2f}s)")
            
            # ìºì‹œì— ì €ì¥
            if target_model_type == "kogpt2":
                _gen_tok_kogpt2 = tok
                _gen_model_kogpt2 = model
            else:
                _gen_tok_solar = tok
                _gen_model_solar = model
            _gen_tok = tok  # í•˜ìœ„ í˜¸í™˜ì„±
            _gen_model = model
        except Exception as e:
            print(f"[_load_poem_model] âŒ CPU ëª¨ë“œ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            traceback.print_exc()
            raise Exception(f"CPU ëª¨ë“œ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {str(e)[:200]}")

    print(f"[_load_poem_model] ì´ ë¡œë”© ì‹œê°„: {time.time() - start_all:.2f}s")
    return tok, model


# ì™¸ë¶€ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í•¨ìˆ˜ export
__all__ = ['_load_poem_model', '_is_gpu', '_device_info']

