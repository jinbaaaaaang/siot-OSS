# -*- coding: utf-8 -*-
"""
Colabì—ì„œ í•™ìŠµí•œ ëª¨ë¸ì„ ë¡œì»¬ì—ì„œ ì‚¬ìš©í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš© ë°©ë²•:
1. Colabì—ì„œ í•™ìŠµí•œ ëª¨ë¸ í´ë”ë¥¼ ë‹¤ìš´ë¡œë“œ
2. ì•„ë˜ ì½”ë“œì—ì„œ model_pathë¥¼ ëª¨ë¸ í´ë” ê²½ë¡œë¡œ ì„¤ì •
3. ì‹¤í–‰
"""

import torch
import re
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForCausalLM

# í‚¤ì›Œë“œ ì¶”ì¶œì„ ìœ„í•œ import (ì„ íƒì )
try:
    import sys
    backend_path = Path(__file__).parent
    sys.path.insert(0, str(backend_path))
    from app.services.keyword_extractor import extract_keywords
    HAS_KEYWORD_EXTRACTOR = True
except ImportError:
    HAS_KEYWORD_EXTRACTOR = False

# ===== ì„¤ì • =====
# Colabì—ì„œ ë‹¤ìš´ë¡œë“œí•œ ëª¨ë¸ í´ë” ê²½ë¡œ ì„¤ì •
MODEL_PATH = "./trained_models/kogpt2_finetuned_fold1_20251109_084450"  # ì‹¤ì œ ëª¨ë¸ ê²½ë¡œë¡œ ë³€ê²½í•˜ì„¸ìš”

# CPU ê°•ì œ ì‚¬ìš© ì—¬ë¶€ (Trueë¡œ ì„¤ì •í•˜ë©´ GPUê°€ ìˆì–´ë„ CPU ì‚¬ìš©)
FORCE_CPU = False  # CPUë§Œ ì‚¬ìš©í•˜ë ¤ë©´ Trueë¡œ ë³€ê²½

# GPU ì‚¬ìš© ì—¬ë¶€ (ìë™ ê°ì§€, FORCE_CPUê°€ Trueë©´ ë¬´ì‹œë¨)
USE_GPU = torch.cuda.is_available() or (hasattr(torch.backends, 'mps') and torch.backends.mps.is_available())


def load_trained_model(model_path: str):
    """í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ"""
    print(f"\n{'='*80}")
    print(f"ëª¨ë¸ ë¡œë”©: {model_path}")
    print(f"{'='*80}\n")
    
    # ë””ë°”ì´ìŠ¤ ì„ íƒ
    if FORCE_CPU:
        device = "cpu"
        dtype = torch.float32
        print(f"ğŸ”§ CPU ê°•ì œ ì‚¬ìš© ëª¨ë“œ")
    elif torch.cuda.is_available():
        device = "cuda"
        dtype = torch.float32
        print(f"âœ… GPU ì‚¬ìš©: {torch.cuda.get_device_name(0)}")
    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        device = "mps"
        dtype = torch.float32
        print(f"âœ… Apple Silicon GPU ì‚¬ìš©")
    else:
        device = "cpu"
        dtype = torch.float32
        print(f"âš ï¸ CPU ëª¨ë“œ (GPU ì—†ìŒ, ëŠë¦¼)")
    
    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    print(f"[1/2] í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...")
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    
    # pad_token ì„¤ì •
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id
    
    print(f"âœ… í† í¬ë‚˜ì´ì € ë¡œë”© ì™„ë£Œ")
    
    # ëª¨ë¸ ë¡œë“œ
    print(f"[2/2] ëª¨ë¸ ë¡œë”© ì¤‘...")
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=dtype
    )
    model = model.to(device).eval()
    print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ (ë””ë°”ì´ìŠ¤: {device})\n")
    
    return tokenizer, model, device


def generate_poem_from_prose(
    prose_text: str,
    tokenizer: AutoTokenizer,
    model: AutoModelForCausalLM,
    device: str,
    max_new_tokens: int = 150  # ì‹œ ê¸¸ì´ ì¡°ì ˆ: 100 â†’ 150 (ì ë‹¹í•œ ê¸¸ì´ì˜ ì‹œ ìƒì„±)
) -> str:
    """
    ì‚°ë¬¸ì„ ì…ë ¥ë°›ì•„ ì‹œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    í•™ìŠµ í˜•ì‹: "ì‚°ë¬¸: [ë‚´ìš©]\nì‹œ: [ë‚´ìš©]"
    ë”°ë¼ì„œ ì…ë ¥ì€ "ì‚°ë¬¸: [ë‚´ìš©]\nì‹œ: " í˜•ì‹ìœ¼ë¡œ ì œê³µ
    """
    # í•™ìŠµ ì‹œ ì‚¬ìš©í•œ í˜•ì‹ìœ¼ë¡œ ì…ë ¥ êµ¬ì„±
    # í•™ìŠµ í˜•ì‹: "ì‚°ë¬¸: [ë‚´ìš©]\nì‹œ: [ì‹œ ë‚´ìš©]"
    # ë”°ë¼ì„œ ì…ë ¥ì€ "ì‚°ë¬¸: [ë‚´ìš©]\nì‹œ: " í˜•ì‹ìœ¼ë¡œ ì œê³µ
    # ì›ë¬¸ì˜ ì£¼ì œë¥¼ ë” ì˜ ë°˜ì˜í•˜ë„ë¡ í”„ë¡¬í”„íŠ¸ ê°•í™”
    
    # ì›ë¬¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ (ì„ íƒì , í‚¤ì›Œë“œ ì¶”ì¶œê¸°ê°€ ìˆìœ¼ë©´ ì‚¬ìš©)
    enhanced_prose = prose_text.strip()
    keyword_text = ""
    if HAS_KEYWORD_EXTRACTOR and len(prose_text) > 10:
        try:
            # ì›ë¬¸ì—ì„œ ì¤‘ìš”í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ìµœëŒ€ 5ê°œ)
            keywords = extract_keywords(prose_text, max_keywords=5)
            if keywords:
                # í‚¤ì›Œë“œë¥¼ í”„ë¡¬í”„íŠ¸ì— í¬í•¨í•˜ì—¬ ì£¼ì œ ì¼ê´€ì„± ê°•í™”
                keyword_text = ", ".join(keywords[:3])  # ìƒìœ„ 3ê°œë§Œ ì‚¬ìš©
        except Exception:
            # í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ ì›ë¬¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            pass
    
    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±: ì›ë¬¸ì˜ ì˜ë¯¸ë¥¼ ë” ê°•ì¡°
    # ì›ë¬¸ì˜ ë‚´ìš©ì„ ëª…í™•íˆ ë°˜ì˜í•˜ë„ë¡ í”„ë¡¬í”„íŠ¸ ê°•í™”
    # í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ê°„ê²°í•˜ê²Œ ì£¼ì œ ê°•ì¡°
    if keyword_text:
        # í‚¤ì›Œë“œë¥¼ í”„ë¡¬í”„íŠ¸ì— ëª…í™•íˆ í¬í•¨í•˜ì—¬ ì£¼ì œ ì¼ê´€ì„± ê°•í™”
        enhanced_prose = f"{prose_text.strip()}\nì£¼ì œ: {keyword_text}"
    else:
        # í‚¤ì›Œë“œê°€ ì—†ì–´ë„ ì›ë¬¸ ë‚´ìš©ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        enhanced_prose = prose_text.strip()
    
    input_text = f"ì‚°ë¬¸: {enhanced_prose}\nì‹œ: "
    
    # í† í¬ë‚˜ì´ì¦ˆ
    enc_ids = tokenizer.encode(input_text, return_tensors="pt").to(device)
    prompt_length = enc_ids.shape[1]
    
    # attention_mask ìƒì„± (ê²½ê³  ë°©ì§€)
    attention_mask = torch.ones_like(enc_ids)
    
    # ì…ë ¥ í† í° ê¸¸ì´ ì œí•œ
    max_pos_embeddings = getattr(model.config, 'max_position_embeddings', 1024)
    safe_max_input = max_pos_embeddings - 100
    if enc_ids.shape[1] >= safe_max_input:
        enc_ids = enc_ids[:, :safe_max_input]
        prompt_length = enc_ids.shape[1]
    
    # ì‹œ ìƒì„± (ì‚°ë¬¸ì´ ì•„ë‹Œ ì‹œë¥¼ ìƒì„±í•˜ë„ë¡ íŒŒë¼ë¯¸í„° ì¡°ì •)
    # ì›ë¬¸ì˜ ì£¼ì œë¥¼ ë” ì˜ ë°˜ì˜í•˜ë„ë¡ temperatureë¥¼ ë” ë‚®ì¶¤
    with torch.no_grad():
        output = model.generate(
            enc_ids,
            attention_mask=attention_mask,
            max_new_tokens=max_new_tokens,
            temperature=0.6,  # 0.65 â†’ 0.6 (ë” ì¼ê´€ì„± ìˆê²Œ, ì›ë¬¸ ì£¼ì œ ìœ ì§€ ê°•í™”)
            top_p=0.8,  # 0.85 â†’ 0.8 (ë” ì§‘ì¤‘ì ì¸ ìƒ˜í”Œë§ìœ¼ë¡œ ì›ë¬¸ ì£¼ì œ ì¼ê´€ì„± í–¥ìƒ)
            top_k=30,  # 35 â†’ 30 (ë” ì œí•œì ì¸ í† í° ì„ íƒìœ¼ë¡œ ì›ë¬¸ ë‚´ìš© ë°˜ì˜ ê°•í™”)
            repetition_penalty=1.6,  # ì‚°ë¬¸ ë°˜ë³µ ë°©ì§€
            no_repeat_ngram_size=5,  # ë” ê¸´ ë°˜ë³µ ë°©ì§€
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id,
            early_stopping=True,  # EOS í† í° ê°ì§€ ì‹œ ì¦‰ì‹œ ì¤‘ë‹¨
        )
    
    # ë””ì½”ë”©
    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
    
    # í”„ë¡¬í”„íŠ¸ ì œê±° (í† í° ê¸°ì¤€ìœ¼ë¡œ ì œê±°)
    if len(output[0]) > prompt_length:
        generated_tokens = output[0][prompt_length:]
        poem = tokenizer.decode(generated_tokens, skip_special_tokens=True)
    else:
        # í† í° ê¸°ì¤€ ì œê±°ê°€ ì•ˆ ë˜ë©´ í…ìŠ¤íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ì‹œë„
        if "ì‹œ: " in generated_text:
            # ë§ˆì§€ë§‰ "ì‹œ: " ì´í›„ë§Œ ì¶”ì¶œ
            parts = generated_text.split("ì‹œ: ")
            if len(parts) > 1:
                poem = parts[-1].strip()  # ë§ˆì§€ë§‰ ë¶€ë¶„ë§Œ ì‚¬ìš©
            else:
                poem = generated_text.strip()
        else:
            poem = generated_text.strip()
    
    # í”„ë¡¬í”„íŠ¸ íŒ¨í„´ ì œê±° (í˜¹ì‹œ ëª¨ë¥¼ ê²½ìš°ë¥¼ ëŒ€ë¹„)
    # "ì‹œ: "ë¡œ ì‹œì‘í•˜ëŠ” ì¤„ ì œê±°, ë°˜ë³µë˜ëŠ” "ì‹œ: " íŒ¨í„´ ì œê±°
    prompt_patterns = [
        r'^ì‹œ:\s*',  # ì¤„ ì‹œì‘ì˜ "ì‹œ: " ì œê±°
        r'ì‹œ:\s*ì‹œ:\s*',  # ë°˜ë³µë˜ëŠ” "ì‹œ: ì‹œ: " ì œê±°
        r'ì‚°ë¬¸:.*?\n',  # "ì‚°ë¬¸: ..." íŒ¨í„´ ì œê±°
        r'\s*ì‹œ:\s*',  # ì¤‘ê°„ì— ì‚½ì…ëœ "ì‹œ: " ì œê±° (ì•ë’¤ ê³µë°± í¬í•¨)
        r'ì‹œ:\s*',  # ëª¨ë“  "ì‹œ: " íŒ¨í„´ ì œê±° (ì•ë’¤ ê³µë°± ì—†ì´ë„)
    ]
    
    for pattern in prompt_patterns:
        poem = re.sub(pattern, '', poem, flags=re.IGNORECASE | re.MULTILINE)
    
    poem = poem.strip()
    
    # í›„ì²˜ë¦¬: ë¹ˆ ì¤„ ì œê±° ë° í”„ë¡¬í”„íŠ¸ê°€ í¬í•¨ëœ ì¤„ ì œê±°
    poem_lines = []
    for line in poem.split('\n'):
        line = line.strip()
        if not line:
            continue
        
        # "ì‹œ:" íŒ¨í„´ì´ ì¤‘ê°„ì— ìˆìœ¼ë©´ ì œê±°
        # "ì‹œ: " ë˜ëŠ” "ì‹œ:" íŒ¨í„´ì„ ëª¨ë‘ ì œê±°
        line = re.sub(r'ì‹œ:\s*', '', line)  # "ì‹œ: " ë˜ëŠ” "ì‹œ:" ì œê±°
        line = re.sub(r'\s*ì‹œ:\s*', '', line)  # ì•ë’¤ ê³µë°± í¬í•¨ "ì‹œ:" ì œê±°
        
        # í”„ë¡¬í”„íŠ¸ê°€ í¬í•¨ëœ ì¤„ ì œê±°
        if any(keyword in line for keyword in ['ì‚°ë¬¸:', 'Write a Korean poem', 'Poem:', '**CRITICAL']):
            # í”„ë¡¬í”„íŠ¸ íŒ¨í„´ì€ ì œê±°
            continue
        
        # "ì‹œ:" í‚¤ì›Œë“œë§Œ ë‚¨ì€ ê²½ìš° ì œê±°
        if line.strip() in ['ì‹œ', 'ì‹œ:', 'ì‹œ: ']:
            continue
        
        if line:  # ë‚´ìš©ì´ ìˆìœ¼ë©´ ì¶”ê°€
            poem_lines.append(line)
    
    poem = '\n'.join(poem_lines)
    
    # ë¬¸ì¥ ë ë° ì‰¼í‘œì—ì„œ ìë™ ì¤„ë°”ê¿ˆ ì²˜ë¦¬
    # í•œ ì¤„ì— ì—¬ëŸ¬ ë¬¸ì¥ì´ ìˆëŠ” ê²½ìš° ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ì—¬ ê° ë¬¸ì¥ì„ í•œ ì¤„ì”© ë°°ì¹˜
    # ë¬¸ì¥ ë íŒ¨í„´: ë§ˆì¹¨í‘œ, ëŠë‚Œí‘œ, ë¬¼ìŒí‘œ, "~ë‹¤", "~ìš”", "~ë¼", "~ì•„", "~ì–´", "~ì˜¤", "~ìš°" ë“±
    # ì‰¼í‘œì—ì„œë„ ì¤„ë°”ê¿ˆ ì¶”ê°€
    sentence_end_markers = ['.', '!', '?', 'ë‹¤', 'ìš”', 'ë¼', 'ì•„', 'ì–´', 'ì˜¤', 'ìš°', 'ë„¤', 'ì£ ']
    comma_markers = [',', 'ï¼Œ']  # ì‰¼í‘œ (í•œê¸€, ì˜ë¬¸)
    
    # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
    sentences = []
    current_sentence = ""
    
    # ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬ëœ ë‹¨ì–´ë“¤ì„ ìˆœíšŒí•˜ë©´ì„œ ë¬¸ì¥ ëì„ ì°¾ìŒ
    words = poem.split()
    for word in words:
        # "ì‹œ:" íŒ¨í„´ì´ í¬í•¨ëœ ë‹¨ì–´ëŠ” ì œê±°
        if 'ì‹œ:' in word:
            word = word.replace('ì‹œ:', '').replace('ì‹œ', '')
            if not word:  # ë‹¨ì–´ê°€ ëª¨ë‘ ì œê±°ë˜ë©´ ìŠ¤í‚µ
                continue
        
        current_sentence += word + " "
        
        # ì‰¼í‘œ ë§ˆì»¤ í™•ì¸ (ë¬¸ì¥ ëë³´ë‹¤ ë¨¼ì € ì²´í¬)
        is_comma = False
        for marker in comma_markers:
            if word.endswith(marker):
                is_comma = True
                break
        
        # ë¬¸ì¥ ë ë§ˆì»¤ í™•ì¸
        is_sentence_end = False
        for marker in sentence_end_markers:
            if word.endswith(marker):
                is_sentence_end = True
                break
        
        # ì‰¼í‘œê°€ ìˆìœ¼ë©´ ì¤„ë°”ê¿ˆ (ë¬¸ì¥ ëë³´ë‹¤ ìš°ì„ )
        if is_comma:
            cleaned_sentence = current_sentence.strip()
            # "ì‹œ:" íŒ¨í„´ì´ ë‚¨ì•„ìˆìœ¼ë©´ ì œê±°
            cleaned_sentence = re.sub(r'\s*ì‹œ:\s*', '', cleaned_sentence)
            if cleaned_sentence:  # ë‚´ìš©ì´ ìˆìœ¼ë©´ ì¶”ê°€
                sentences.append(cleaned_sentence)
            current_sentence = ""
        # ë¬¸ì¥ì´ ëë‚˜ë©´ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•˜ê³  ìƒˆ ë¬¸ì¥ ì‹œì‘
        elif is_sentence_end:
            cleaned_sentence = current_sentence.strip()
            # "ì‹œ:" íŒ¨í„´ì´ ë‚¨ì•„ìˆìœ¼ë©´ ì œê±°
            cleaned_sentence = re.sub(r'\s*ì‹œ:\s*', '', cleaned_sentence)
            if cleaned_sentence:  # ë‚´ìš©ì´ ìˆìœ¼ë©´ ì¶”ê°€
                sentences.append(cleaned_sentence)
            current_sentence = ""
    
    # ë§ˆì§€ë§‰ ë¬¸ì¥ì´ ìˆìœ¼ë©´ ì¶”ê°€
    if current_sentence.strip():
        cleaned_sentence = current_sentence.strip()
        # "ì‹œ:" íŒ¨í„´ì´ ë‚¨ì•„ìˆìœ¼ë©´ ì œê±°
        cleaned_sentence = re.sub(r'\s*ì‹œ:\s*', '', cleaned_sentence)
        if cleaned_sentence:
            sentences.append(cleaned_sentence)
    
    # ê° ë¬¸ì¥ì„ í•œ ì¤„ì”© ë°°ì¹˜
    if sentences:
        poem = '\n'.join(sentences)
    else:
        # ë¬¸ì¥ ë¶„ë¦¬ê°€ ì•ˆ ë˜ë©´ ì›ë³¸ ìœ ì§€
        pass
    
    # ì‚°ë¬¸ íŒ¨í„´ ì œê±° ë° ì‹œ í˜•ì‹ ê°•í™”
    # ì‚°ë¬¸ íŠ¹ì§•: ê¸´ ë¬¸ì¥, "~ë‹¤", "~ìš”", "~ì´ë‹¤" ë“±ìœ¼ë¡œ ëë‚˜ëŠ” ë¬¸ì¥, ì‰¼í‘œê°€ ë§ì€ ë¬¸ì¥
    prose_indicators = [
        r'[ê°€-í£]{10,}ë‹¤\.',  # 10ì ì´ìƒ + "ë‹¤." íŒ¨í„´ (ì˜ˆ: "ì˜¤ëŠ˜ì€ ì •ë§ ì¢‹ì€ í•˜ë£¨ì˜€ë‹¤.")
        r'[ê°€-í£]{10,}ìš”\.',  # 10ì ì´ìƒ + "ìš”." íŒ¨í„´
        r'[ê°€-í£]{10,}ì´ë‹¤\.',  # 10ì ì´ìƒ + "ì´ë‹¤." íŒ¨í„´
        r'[ê°€-í£]{15,}[,ï¼Œ]',  # 15ì ì´ìƒ + ì‰¼í‘œ (ì‚°ë¬¸ íŠ¹ì§•)
        r'[ê°€-í£]{20,}',  # 20ì ì´ìƒì˜ ê¸´ ë¬¸ì¥ (ì‹œëŠ” ë³´í†µ ì§§ìŒ)
    ]
    
    # ë‰´ìŠ¤ ê¸°ì‚¬ë‚˜ ì‹¤ì œ ì •ë³´ë¥¼ ë‹´ì€ ë¬¸ì¥ íŒ¨í„´ (ì œê±° ëŒ€ìƒ)
    news_info_keywords = [
        'í•œêµ­ë„ë¡œê³µì‚¬', 'ì„œìš¸í†¨ê²Œì´íŠ¸', 'ì— ë”°ë¥´ë©´', 'ì´ë‚ ', 'ì˜¤í›„', 'ì˜¤ì „', 'ì‹œë¶€í„°', 'ì‹œê¹Œì§€',
        'ê³ ì†ë„ë¡œ', 'ë¶„ê¸°ì ', 'ë°©í–¥', 'í†¨ê²Œì´íŠ¸', 'ê³µì‚¬', 'ê±´ì„¤', 'ë°œí‘œ', 'ë°œí‘œì— ë”°ë¥´ë©´',
        'ë³´ë„ì— ë”°ë¥´ë©´', 'ê´€ê³„ì', 'ë‹¹êµ­', 'ê¸°ê´€', 'ë¶€ì„œ', 'ì²­', 'ì‹œì²­', 'êµ¬ì²­',
        'ê²½ë¶€', 'ê²½ì¸', 'ì„œí•´ì•ˆ', 'ì¤‘ì•™', 'ì˜ë™', 'í˜¸ë‚¨', 'ì¤‘ë¶€', 'ë™í•´',
        'ë¶€ì‚°ë°©í–¥', 'ì„œìš¸ë°©í–¥', 'ì¸ì²œë°©í–¥', 'ëŒ€ì „ë°©í–¥', 'ëŒ€êµ¬ë°©í–¥',
        'ì‹ ê°ˆ', 'ì•ˆì„±', 'í•œë‚¨', 'íŒêµ', 'ê¸°í¥', 'ìˆ˜ì›', 'ìš©ì¸',
        'km', 'm', 'km/h', 'ì›', 'ë§Œì›', 'ì–µì›',
        'ë…„', 'ì›”', 'ì¼', 'ì‹œ', 'ë¶„',
        'ì „ë§', 'ì˜ˆìƒ', 'ê³„íš', 'ì¶”ì§„', 'ê²€í† ', 'ë…¼ì˜',
    ]
    
    # ì‚°ë¬¸ì²˜ëŸ¼ ë³´ì´ëŠ” ë¬¸ì¥ ë° ë‰´ìŠ¤ ê¸°ì‚¬ ìŠ¤íƒ€ì¼ ë¬¸ì¥ ì œê±°
    filtered_lines = []
    for line in poem.split('\n'):
        line = line.strip()
        if not line:
            continue
        
        # ì‚°ë¬¸ íŒ¨í„´ ì²´í¬
        is_prose = False
        is_news_info = False
        
        # 1. ë‰´ìŠ¤ ê¸°ì‚¬ë‚˜ ì‹¤ì œ ì •ë³´ë¥¼ ë‹´ì€ ë¬¸ì¥ ì²´í¬ (ìš°ì„  ì²˜ë¦¬)
        for keyword in news_info_keywords:
            if keyword in line:
                is_news_info = True
                break
        
        # 2. ë„ˆë¬´ ê¸´ ë¬¸ì¥ (30ì ì´ìƒìœ¼ë¡œ ì™„í™”)
        if len(line) > 30:  # 20 â†’ 30 (ë” ê¸´ ì¤„ í—ˆìš©)
            # ì‚°ë¬¸ íŒ¨í„´ì´ ìˆëŠ”ì§€ í™•ì¸
            for pattern in prose_indicators:
                if re.search(pattern, line):
                    is_prose = True
                    break
            
            # 3. ì‰¼í‘œê°€ 3ê°œ ì´ìƒì¸ ê¸´ ë¬¸ì¥ë„ ì‚°ë¬¸ìœ¼ë¡œ ê°„ì£¼ (2ê°œ â†’ 3ê°œë¡œ ì™„í™”)
            if not is_prose and line.count(',') >= 3 and len(line) > 20:  # 2ê°œ â†’ 3ê°œ, 15ì â†’ 20ì
                is_prose = True
        
        # ë‰´ìŠ¤ ê¸°ì‚¬ ìŠ¤íƒ€ì¼ì´ê±°ë‚˜ ì‚°ë¬¸ì´ ì•„ë‹Œ ê²½ìš°ë§Œ ì¶”ê°€
        if not is_news_info and not is_prose:
            filtered_lines.append(line)
    
    poem = '\n'.join(filtered_lines)
    
    # ë¹ˆ ê²°ê³¼ ì²˜ë¦¬
    if not poem.strip():
        # í•„í„°ë§ì´ ë„ˆë¬´ ê°•í•´ì„œ ëª¨ë“  ì¤„ì´ ì œê±°ëœ ê²½ìš°
        # ì›ë³¸ì—ì„œ ì§§ì€ ì¤„ë§Œ ì„ íƒ
        original_lines = '\n'.join(poem_lines).split('\n')
        short_lines = [line.strip() for line in original_lines if line.strip() and len(line.strip()) <= 30]  # 20 â†’ 30
        if short_lines:
            poem = '\n'.join(short_lines[:15])  # ìµœëŒ€ 10ì¤„ â†’ 15ì¤„
    
    # ì‹œ ê¸¸ì´ ì œí•œ (ë„ˆë¬´ ê¸¸ë©´ ìë™ìœ¼ë¡œ ì˜ë¼ë‚´ê¸°)
    # ë³´í†µ ì‹œëŠ” 15-20ì¤„ ì •ë„ê°€ ì ë‹¹
    max_lines = 20  # 15 â†’ 20 (ë” ê¸´ ì‹œ í—ˆìš©)
    poem_lines_final = poem.split('\n')[:max_lines]
    poem = '\n'.join(poem_lines_final).strip()
    
    # ìµœëŒ€ ë¬¸ì ìˆ˜ ì œí•œ (ì•½ 800ìë¡œ ì¦ê°€)
    # ë¬¸ì¥ì´ ì¤‘ê°„ì— ì˜ë¦¬ì§€ ì•Šë„ë¡ ì²˜ë¦¬
    if len(poem) > 800:  # 500 â†’ 800 (ë” ê¸´ ì‹œ í—ˆìš©)
        lines = poem.split('\n')
        result_lines = []
        total_length = 0
        
        for line in lines:
            line_length = len(line) + 1  # ì¤„ë°”ê¿ˆ í¬í•¨
            if total_length + line_length > 500:
                # ì œí•œì„ ë„˜ìœ¼ë©´ ì¤‘ë‹¨ (ë§ˆì§€ë§‰ ì¤„ì€ ì œì™¸í•˜ì—¬ ë¶ˆì™„ì „í•œ ë¬¸ì¥ ë°©ì§€)
                break
            result_lines.append(line)
            total_length += line_length
        
        poem = '\n'.join(result_lines).strip()
    
    # ë§ˆì§€ë§‰ ë¬¸ì¥ì´ ë¶ˆì™„ì „í•˜ê²Œ ì˜ë ¸ëŠ”ì§€ í™•ì¸ ë° ì²˜ë¦¬
    # ë¬¸ì¥ ë íŒ¨í„´: ë§ˆì¹¨í‘œ, ëŠë‚Œí‘œ, ë¬¼ìŒí‘œ, ì¤„ë°”ê¿ˆ, ë˜ëŠ” ì‹œì  í‘œí˜„ìœ¼ë¡œ ëë‚˜ëŠ” ê²½ìš°
    if poem:
        lines = poem.split('\n')
        if lines:
            last_line = lines[-1].strip()
            
            # ë¶ˆì™„ì „í•œ ë¬¸ì¥ íŒ¨í„´ ì²´í¬
            incomplete_patterns = [
                r'[ê°€-í£]+[ì€ëŠ”ì´ê°€ì„ë¥¼]$',  # ì¡°ì‚¬ë¡œ ëë‚˜ëŠ” ê²½ìš° (ì˜ˆ: "ë‚˜ëŠ”", "ê·¸ëŠ”")
                r'[ê°€-í£]+[ì™€ê³¼]$',  # ì ‘ì†ì¡°ì‚¬ë¡œ ëë‚˜ëŠ” ê²½ìš°
                r'[ê°€-í£]+[ì—ì—ì„œ]$',  # ë¶€ì‚¬ê²© ì¡°ì‚¬ë¡œ ëë‚˜ëŠ” ê²½ìš°
                r'[ê°€-í£]+[ì˜]$',  # ê´€í˜•ê²© ì¡°ì‚¬ë¡œ ëë‚˜ëŠ” ê²½ìš°
                r'[ê°€-í£]+[ë„]$',  # ë³´ì¡°ì‚¬ë¡œ ëë‚˜ëŠ” ê²½ìš°
            ]
            
            is_incomplete = False
            for pattern in incomplete_patterns:
                if re.search(pattern, last_line):
                    is_incomplete = True
                    break
            
            # ë§ˆì§€ë§‰ ì¤„ì´ ë¶ˆì™„ì „í•˜ë©´ ì œê±°
            if is_incomplete and len(lines) > 1:
                lines = lines[:-1]
                poem = '\n'.join(lines).strip()
            
            # ë§ˆì§€ë§‰ ì¤„ì´ ë„ˆë¬´ ì§§ê³ (5ì ì´í•˜) ë¶ˆì™„ì „í•´ ë³´ì´ë©´ ì œê±°
            elif len(last_line) <= 5 and not any(last_line.endswith(c) for c in ['.', '!', '?', 'ë‹¤', 'ìš”', 'ë¼', 'ì•„', 'ì–´', 'ì˜¤', 'ìš°']):
                if len(lines) > 1:
                    lines = lines[:-1]
                    poem = '\n'.join(lines).strip()
    
    return poem


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # ëª¨ë¸ ê²½ë¡œ í™•ì¸
    model_path = Path(MODEL_PATH)
    if not model_path.exists():
        print(f"âŒ ëª¨ë¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {MODEL_PATH}")
        print(f"\nğŸ’¡ ì‚¬ìš© ë°©ë²•:")
        print(f"   1. Colabì—ì„œ í•™ìŠµí•œ ëª¨ë¸ í´ë”ë¥¼ ë‹¤ìš´ë¡œë“œ")
        print(f"   2. ì´ ìŠ¤í¬ë¦½íŠ¸ì˜ MODEL_PATH ë³€ìˆ˜ë¥¼ ëª¨ë¸ í´ë” ê²½ë¡œë¡œ ë³€ê²½")
        print(f"   3. ë‹¤ì‹œ ì‹¤í–‰")
        return
    
    # ëª¨ë¸ ë¡œë“œ
    try:
        tokenizer, model, device = load_trained_model(str(model_path))
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ
    print(f"\n{'='*80}")
    print("ì‹œ ìƒì„± ì‹œì‘ (ì¢…ë£Œí•˜ë ¤ë©´ 'quit' ë˜ëŠ” 'exit' ì…ë ¥)")
    print(f"{'='*80}\n")
    
    while True:
        try:
            # ì‚¬ìš©ì ì…ë ¥
            prose_text = input("ì‚°ë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
            
            if prose_text.lower() in ['quit', 'exit', 'ì¢…ë£Œ', 'q']:
                print("\nì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            
            if not prose_text:
                print("âš ï¸ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
                continue
            
            # ì‹œ ìƒì„±
            print(f"\nì‹œ ìƒì„± ì¤‘...")
            poem = generate_poem_from_prose(prose_text, tokenizer, model, device)
            
            # ê²°ê³¼ ì¶œë ¥
            print(f"\n{'='*80}")
            print("ìƒì„±ëœ ì‹œ:")
            print(f"{'='*80}")
            print(poem)
            print(f"{'='*80}\n")
            
        except KeyboardInterrupt:
            print("\n\nì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
            print()


if __name__ == "__main__":
    main()

